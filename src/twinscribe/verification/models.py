"""
CrossCheck Verification Framework - Data Models.

This module defines Pydantic models for all verification-related data:

Challenge Models:
    - VerificationChallenge: Base class for all challenges
    - Question: Q&A interrogation question
    - MaskedChallenge: Masked code reconstruction challenge
    - Scenario: Execution scenario for walkthrough
    - Mutation: Code mutation for detection tests
    - ImpactChallenge: Impact analysis challenge

Result Models:
    - VerificationResult: Base class for all results
    - QAResult: Q&A evaluation result
    - ReconstructionResult: Masked reconstruction result
    - ScenarioResult: Scenario walkthrough result
    - MutationResult: Mutation detection result
    - ImpactResult: Impact analysis result
    - AdversarialFinding: Issue found in adversarial review
    - TestValidationResult: Test generation validation result

Supporting Models:
    - Mask: A masked element in code
    - DocumentationGap: Identified gap in documentation
    - ExecutionTrace: Predicted execution path
    - TeamAnswer: Answer from a documentation team
"""

from datetime import datetime
from typing import Any

from pydantic import BaseModel, Field, computed_field

from twinscribe.verification.base import (
    ChangeType,
    MaskType,
    MutationType,
    QuestionCategory,
    ScenarioType,
    Severity,
    StrategyType,
)

# =============================================================================
# Base Models
# =============================================================================


class VerificationChallenge(BaseModel):
    """Base class for all verification challenges.

    A challenge is generated by Agent C (examiner) and presented
    to teams A and B who must respond using only their documentation.

    Attributes:
        challenge_id: Unique identifier for this challenge
        component_id: Component being tested
        strategy_type: Which verification strategy generated this
        created_at: When the challenge was created
        metadata: Additional strategy-specific metadata
    """

    challenge_id: str = Field(
        ...,
        description="Unique identifier",
        examples=["chal_qa_001"],
    )
    component_id: str = Field(
        ...,
        min_length=1,
        description="Component being tested",
    )
    strategy_type: StrategyType = Field(
        ...,
        description="Strategy that generated this challenge",
    )
    created_at: datetime = Field(
        default_factory=datetime.utcnow,
        description="Creation timestamp",
    )
    metadata: dict[str, Any] = Field(
        default_factory=dict,
        description="Strategy-specific metadata",
    )


class VerificationResult(BaseModel):
    """Base class for all verification results.

    Results capture the evaluation of team responses against
    the challenge and ground truth.

    Attributes:
        result_id: Unique identifier for this result
        challenge_id: Challenge this result evaluates
        component_id: Component that was tested
        strategy_type: Strategy that produced this result
        team_a_score: Score for Team A (0.0-1.0)
        team_b_score: Score for Team B (0.0-1.0)
        evaluated_at: When evaluation was performed
        documentation_gaps: Gaps identified from both teams
    """

    result_id: str = Field(
        ...,
        description="Unique identifier",
        examples=["res_qa_001"],
    )
    challenge_id: str = Field(
        ...,
        description="Associated challenge ID",
    )
    component_id: str = Field(
        ...,
        min_length=1,
        description="Component that was tested",
    )
    strategy_type: StrategyType = Field(
        ...,
        description="Strategy that produced this result",
    )
    team_a_score: float = Field(
        default=0.0,
        ge=0.0,
        le=1.0,
        description="Team A score",
    )
    team_b_score: float = Field(
        default=0.0,
        ge=0.0,
        le=1.0,
        description="Team B score",
    )
    evaluated_at: datetime = Field(
        default_factory=datetime.utcnow,
        description="Evaluation timestamp",
    )
    documentation_gaps: list["DocumentationGap"] = Field(
        default_factory=list,
        description="Identified documentation gaps",
    )

    @computed_field
    @property
    def average_score(self) -> float:
        """Average score across both teams."""
        return (self.team_a_score + self.team_b_score) / 2

    @computed_field
    @property
    def both_teams_failed(self) -> bool:
        """True if both teams scored below 0.5."""
        return self.team_a_score < 0.5 and self.team_b_score < 0.5


class DocumentationGap(BaseModel):
    """An identified gap in documentation quality.

    Represents a specific area where documentation is insufficient
    to accurately understand, predict, or reconstruct code behavior.

    Attributes:
        gap_id: Unique identifier for this gap
        area: Which documentation aspect is lacking
        description: Detailed description of the gap
        severity: How critical this gap is
        recommendation: Suggested improvement
        evidence: Evidence supporting this gap identification
        affects_team_a: Whether Team A's docs have this gap
        affects_team_b: Whether Team B's docs have this gap
    """

    gap_id: str = Field(
        ...,
        description="Unique identifier",
        examples=["gap_001"],
    )
    area: str = Field(
        ...,
        description="Documentation area lacking",
        examples=["return_value", "exception_handling", "call_graph"],
    )
    description: str = Field(
        ...,
        description="Detailed description of the gap",
    )
    severity: Severity = Field(
        default=Severity.MEDIUM,
        description="Gap severity",
    )
    recommendation: str = Field(
        default="",
        description="Suggested improvement",
    )
    evidence: str = Field(
        default="",
        description="Evidence supporting gap identification",
    )
    affects_team_a: bool = Field(
        default=True,
        description="Team A has this gap",
    )
    affects_team_b: bool = Field(
        default=True,
        description="Team B has this gap",
    )

    @computed_field
    @property
    def is_shared_gap(self) -> bool:
        """True if both teams have this gap."""
        return self.affects_team_a and self.affects_team_b


# =============================================================================
# Q&A Interrogation Models
# =============================================================================


class Question(BaseModel):
    """A verification question generated from code analysis.

    Questions are designed to have definitive answers derivable from
    the code, testing whether documentation captures the behavior.

    Attributes:
        question_id: Unique identifier
        text: The question text
        category: What aspect this tests
        correct_answer: Ground truth answer from code
        gap_indicator: What a wrong answer would indicate
        difficulty: Question difficulty (1-5)
    """

    question_id: str = Field(
        ...,
        description="Unique identifier",
        examples=["q_001"],
    )
    text: str = Field(
        ...,
        min_length=10,
        description="The question text",
        examples=["What happens if process_data() receives an empty list?"],
    )
    category: QuestionCategory = Field(
        ...,
        description="Question category",
    )
    correct_answer: str = Field(
        ...,
        description="Ground truth answer from code analysis",
    )
    gap_indicator: str = Field(
        default="",
        description="What documentation gap a wrong answer indicates",
    )
    difficulty: int = Field(
        default=3,
        ge=1,
        le=5,
        description="Difficulty level (1=easy, 5=hard)",
    )


class QAChallenge(VerificationChallenge):
    """Q&A interrogation challenge.

    Contains multiple questions about a component that teams
    must answer using only their documentation.

    Attributes:
        questions: List of questions to answer
        time_limit_seconds: Optional time limit
    """

    questions: list[Question] = Field(
        default_factory=list,
        description="Questions to answer",
    )
    time_limit_seconds: int | None = Field(
        default=None,
        ge=30,
        description="Optional time limit",
    )

    def __init__(self, **data):
        if "strategy_type" not in data:
            data["strategy_type"] = StrategyType.QA_INTERROGATION
        super().__init__(**data)


class TeamAnswer(BaseModel):
    """An answer provided by a team for a question.

    Attributes:
        question_id: ID of the question answered
        answer: The team's answer
        confidence: Team's confidence in their answer (0.0-1.0)
        reasoning: Optional explanation of their reasoning
    """

    question_id: str = Field(
        ...,
        description="Question ID",
    )
    answer: str = Field(
        ...,
        description="The answer provided",
    )
    confidence: float = Field(
        default=0.5,
        ge=0.0,
        le=1.0,
        description="Answer confidence",
    )
    reasoning: str | None = Field(
        default=None,
        description="Explanation of reasoning",
    )


class QAEvaluation(BaseModel):
    """Evaluation of a single Q&A answer.

    Attributes:
        question_id: Question that was evaluated
        is_correct: Whether the answer was correct
        is_complete: Whether the answer was complete
        semantic_similarity: Similarity to correct answer (0.0-1.0)
        identified_gap: Documentation gap if wrong
    """

    question_id: str = Field(
        ...,
        description="Question ID",
    )
    is_correct: bool = Field(
        default=False,
        description="Whether answer was correct",
    )
    is_complete: bool = Field(
        default=False,
        description="Whether answer was complete",
    )
    semantic_similarity: float = Field(
        default=0.0,
        ge=0.0,
        le=1.0,
        description="Semantic similarity to correct answer",
    )
    identified_gap: str | None = Field(
        default=None,
        description="Documentation gap revealed",
    )


class QAResult(VerificationResult):
    """Result of Q&A interrogation evaluation.

    Attributes:
        team_a_evaluations: Evaluation of each Team A answer
        team_b_evaluations: Evaluation of each Team B answer
        questions_correct_both: Questions both teams got right
        questions_wrong_both: Questions both teams got wrong
        category_scores: Scores broken down by question category
    """

    team_a_evaluations: list[QAEvaluation] = Field(
        default_factory=list,
        description="Team A answer evaluations",
    )
    team_b_evaluations: list[QAEvaluation] = Field(
        default_factory=list,
        description="Team B answer evaluations",
    )
    questions_correct_both: list[str] = Field(
        default_factory=list,
        description="Question IDs both teams answered correctly",
    )
    questions_wrong_both: list[str] = Field(
        default_factory=list,
        description="Question IDs both teams answered incorrectly",
    )
    category_scores: dict[str, float] = Field(
        default_factory=dict,
        description="Scores by question category",
    )

    def __init__(self, **data):
        if "strategy_type" not in data:
            data["strategy_type"] = StrategyType.QA_INTERROGATION
        super().__init__(**data)

    @computed_field
    @property
    def shared_knowledge_gaps(self) -> int:
        """Number of questions both teams got wrong."""
        return len(self.questions_wrong_both)


# =============================================================================
# Masked Reconstruction Models
# =============================================================================


class Mask(BaseModel):
    """A masked element in code.

    Represents a portion of code that has been hidden for
    reconstruction testing.

    Attributes:
        mask_id: Unique identifier for this mask
        start: Starting character position
        end: Ending character position
        original: The original (hidden) value
        mask_type: Type of element masked
        placeholder: What's shown in place of original
    """

    mask_id: str = Field(
        ...,
        description="Unique identifier",
        examples=["mask_001"],
    )
    start: int = Field(
        ...,
        ge=0,
        description="Start position in code",
    )
    end: int = Field(
        ...,
        ge=0,
        description="End position in code",
    )
    original: str = Field(
        ...,
        description="Original hidden value",
    )
    mask_type: MaskType = Field(
        ...,
        description="Type of masked element",
    )
    placeholder: str = Field(
        default="████████",
        description="Placeholder text shown",
    )


class MaskedChallenge(VerificationChallenge):
    """Masked code reconstruction challenge.

    Contains code with masked portions that teams must
    reconstruct using only their documentation.

    Attributes:
        original_code: Full original source code
        masked_code: Code with masks applied
        masks: List of applied masks
        mask_ratio: Ratio of code that was masked
    """

    original_code: str = Field(
        ...,
        description="Original source code",
    )
    masked_code: str = Field(
        ...,
        description="Code with masks applied",
    )
    masks: list[Mask] = Field(
        default_factory=list,
        description="Applied masks",
    )
    mask_ratio: float = Field(
        default=0.3,
        ge=0.0,
        le=1.0,
        description="Ratio of code masked",
    )

    def __init__(self, **data):
        if "strategy_type" not in data:
            data["strategy_type"] = StrategyType.MASKED_RECONSTRUCTION
        super().__init__(**data)


class MaskReconstruction(BaseModel):
    """A team's reconstruction of a masked element.

    Attributes:
        mask_id: ID of the mask being reconstructed
        reconstructed_value: Team's guess at the original
        confidence: Team's confidence (0.0-1.0)
    """

    mask_id: str = Field(
        ...,
        description="Mask ID",
    )
    reconstructed_value: str = Field(
        ...,
        description="Reconstructed value",
    )
    confidence: float = Field(
        default=0.5,
        ge=0.0,
        le=1.0,
        description="Reconstruction confidence",
    )


class MaskEvaluation(BaseModel):
    """Evaluation of a mask reconstruction.

    Attributes:
        mask_id: Mask that was evaluated
        is_correct: Whether reconstruction matches original
        is_semantically_equivalent: Whether functionally equivalent
        similarity_score: How similar to original (0.0-1.0)
    """

    mask_id: str = Field(
        ...,
        description="Mask ID",
    )
    is_correct: bool = Field(
        default=False,
        description="Exact match with original",
    )
    is_semantically_equivalent: bool = Field(
        default=False,
        description="Functionally equivalent",
    )
    similarity_score: float = Field(
        default=0.0,
        ge=0.0,
        le=1.0,
        description="Similarity to original",
    )


class ReconstructionResult(VerificationResult):
    """Result of masked reconstruction evaluation.

    Attributes:
        team_a_evaluations: Evaluation of Team A reconstructions
        team_b_evaluations: Evaluation of Team B reconstructions
        masks_correct_both: Masks both teams reconstructed correctly
        masks_wrong_both: Masks both teams failed to reconstruct
        mask_type_scores: Scores broken down by mask type
    """

    team_a_evaluations: list[MaskEvaluation] = Field(
        default_factory=list,
        description="Team A reconstruction evaluations",
    )
    team_b_evaluations: list[MaskEvaluation] = Field(
        default_factory=list,
        description="Team B reconstruction evaluations",
    )
    masks_correct_both: list[str] = Field(
        default_factory=list,
        description="Mask IDs both teams got correct",
    )
    masks_wrong_both: list[str] = Field(
        default_factory=list,
        description="Mask IDs both teams got wrong",
    )
    mask_type_scores: dict[str, float] = Field(
        default_factory=dict,
        description="Scores by mask type",
    )

    def __init__(self, **data):
        if "strategy_type" not in data:
            data["strategy_type"] = StrategyType.MASKED_RECONSTRUCTION
        super().__init__(**data)


# =============================================================================
# Scenario Walkthrough Models
# =============================================================================


class Scenario(BaseModel):
    """An execution scenario for walkthrough testing.

    Scenarios define inputs and ask teams to predict execution
    behavior using only their documentation.

    Attributes:
        scenario_id: Unique identifier
        scenario_type: Type of scenario
        description: Scenario description
        inputs: Input values/state
        expected_calls: Expected function call sequence
        expected_output: Expected return value or exception
        expected_side_effects: Expected side effects
    """

    scenario_id: str = Field(
        ...,
        description="Unique identifier",
        examples=["scen_001"],
    )
    scenario_type: ScenarioType = Field(
        ...,
        description="Type of scenario",
    )
    description: str = Field(
        ...,
        description="Scenario description",
        examples=["User calls api.create_order(items=[], user_id=None)"],
    )
    inputs: dict[str, Any] = Field(
        default_factory=dict,
        description="Input values and state",
    )
    expected_calls: list[str] = Field(
        default_factory=list,
        description="Expected call sequence",
    )
    expected_output: str | None = Field(
        default=None,
        description="Expected return or exception",
    )
    expected_side_effects: list[str] = Field(
        default_factory=list,
        description="Expected side effects",
    )


class ScenarioChallenge(VerificationChallenge):
    """Scenario walkthrough challenge.

    Contains scenarios for teams to trace using their documentation.

    Attributes:
        scenarios: List of scenarios to trace
        include_side_effects: Whether to test side effect prediction
    """

    scenarios: list[Scenario] = Field(
        default_factory=list,
        description="Scenarios to trace",
    )
    include_side_effects: bool = Field(
        default=True,
        description="Test side effect prediction",
    )

    def __init__(self, **data):
        if "strategy_type" not in data:
            data["strategy_type"] = StrategyType.SCENARIO_WALKTHROUGH
        super().__init__(**data)


class ExecutionTrace(BaseModel):
    """A team's predicted execution trace.

    Attributes:
        scenario_id: Scenario being traced
        predicted_calls: Predicted call sequence
        predicted_output: Predicted return or exception
        predicted_side_effects: Predicted side effects
        confidence: Team's confidence (0.0-1.0)
    """

    scenario_id: str = Field(
        ...,
        description="Scenario ID",
    )
    predicted_calls: list[str] = Field(
        default_factory=list,
        description="Predicted call sequence",
    )
    predicted_output: str | None = Field(
        default=None,
        description="Predicted return or exception",
    )
    predicted_side_effects: list[str] = Field(
        default_factory=list,
        description="Predicted side effects",
    )
    confidence: float = Field(
        default=0.5,
        ge=0.0,
        le=1.0,
        description="Prediction confidence",
    )


class ScenarioEvaluation(BaseModel):
    """Evaluation of a scenario trace.

    Attributes:
        scenario_id: Scenario that was evaluated
        call_sequence_score: How accurate the call sequence was
        output_correct: Whether output prediction was correct
        side_effects_score: How accurate side effect prediction was
        overall_score: Combined accuracy score
        missed_calls: Calls that were missed
        missed_side_effects: Side effects that were missed
    """

    scenario_id: str = Field(
        ...,
        description="Scenario ID",
    )
    call_sequence_score: float = Field(
        default=0.0,
        ge=0.0,
        le=1.0,
        description="Call sequence accuracy",
    )
    output_correct: bool = Field(
        default=False,
        description="Output prediction correct",
    )
    side_effects_score: float = Field(
        default=0.0,
        ge=0.0,
        le=1.0,
        description="Side effects accuracy",
    )
    overall_score: float = Field(
        default=0.0,
        ge=0.0,
        le=1.0,
        description="Combined accuracy",
    )
    missed_calls: list[str] = Field(
        default_factory=list,
        description="Missed function calls",
    )
    missed_side_effects: list[str] = Field(
        default_factory=list,
        description="Missed side effects",
    )


class ScenarioResult(VerificationResult):
    """Result of scenario walkthrough evaluation.

    Attributes:
        team_a_evaluations: Evaluation of Team A traces
        team_b_evaluations: Evaluation of Team B traces
        scenarios_correct_both: Scenarios both teams traced correctly
        scenarios_wrong_both: Scenarios both teams traced incorrectly
        commonly_missed_calls: Calls both teams missed
        commonly_missed_side_effects: Side effects both teams missed
    """

    team_a_evaluations: list[ScenarioEvaluation] = Field(
        default_factory=list,
        description="Team A trace evaluations",
    )
    team_b_evaluations: list[ScenarioEvaluation] = Field(
        default_factory=list,
        description="Team B trace evaluations",
    )
    scenarios_correct_both: list[str] = Field(
        default_factory=list,
        description="Scenario IDs both teams got correct",
    )
    scenarios_wrong_both: list[str] = Field(
        default_factory=list,
        description="Scenario IDs both teams got wrong",
    )
    commonly_missed_calls: list[str] = Field(
        default_factory=list,
        description="Calls missed by both teams",
    )
    commonly_missed_side_effects: list[str] = Field(
        default_factory=list,
        description="Side effects missed by both teams",
    )

    def __init__(self, **data):
        if "strategy_type" not in data:
            data["strategy_type"] = StrategyType.SCENARIO_WALKTHROUGH
        super().__init__(**data)


# =============================================================================
# Mutation Detection Models
# =============================================================================


class Mutation(BaseModel):
    """A code mutation for detection testing.

    Mutations introduce subtle bugs that should be detectable
    from precise documentation.

    Attributes:
        mutation_id: Unique identifier
        mutation_type: Type of mutation
        original_code: Code before mutation
        mutated_code: Code after mutation
        description: Description of what changed
        line_number: Line where mutation occurs
        detection_hint: What documentation would catch this
    """

    mutation_id: str = Field(
        ...,
        description="Unique identifier",
        examples=["mut_001"],
    )
    mutation_type: MutationType = Field(
        ...,
        description="Type of mutation",
    )
    original_code: str = Field(
        ...,
        description="Original code snippet",
    )
    mutated_code: str = Field(
        ...,
        description="Mutated code snippet",
    )
    description: str = Field(
        ...,
        description="Description of the mutation",
        examples=["Changed >= to > in balance check"],
    )
    line_number: int = Field(
        ...,
        ge=1,
        description="Line where mutation occurs",
    )
    detection_hint: str = Field(
        default="",
        description="What documentation would help detect this",
    )


class MutationChallenge(VerificationChallenge):
    """Mutation detection challenge.

    Contains mutations that teams must assess for detectability
    using their documentation.

    Attributes:
        mutations: List of mutations to assess
        full_mutated_code: Complete code with mutations applied
    """

    mutations: list[Mutation] = Field(
        default_factory=list,
        description="Mutations to assess",
    )
    full_mutated_code: str = Field(
        default="",
        description="Complete mutated code",
    )

    def __init__(self, **data):
        if "strategy_type" not in data:
            data["strategy_type"] = StrategyType.MUTATION_DETECTION
        super().__init__(**data)


class MutationAssessment(BaseModel):
    """A team's assessment of mutation detectability.

    Attributes:
        mutation_id: Mutation being assessed
        would_detect: Whether docs would help detect this
        relevant_documentation: Which docs would help
        confidence: Team's confidence (0.0-1.0)
        explanation: Explanation of assessment
    """

    mutation_id: str = Field(
        ...,
        description="Mutation ID",
    )
    would_detect: bool = Field(
        ...,
        description="Whether docs would detect mutation",
    )
    relevant_documentation: str = Field(
        default="",
        description="Documentation that would help",
    )
    confidence: float = Field(
        default=0.5,
        ge=0.0,
        le=1.0,
        description="Assessment confidence",
    )
    explanation: str = Field(
        default="",
        description="Explanation of assessment",
    )


class MutationEvaluation(BaseModel):
    """Evaluation of a mutation assessment.

    Attributes:
        mutation_id: Mutation that was evaluated
        assessment_accurate: Whether the assessment was accurate
        docs_actually_help: Whether docs actually help detect
        precision_gap: If docs are too vague to detect
    """

    mutation_id: str = Field(
        ...,
        description="Mutation ID",
    )
    assessment_accurate: bool = Field(
        default=False,
        description="Assessment was accurate",
    )
    docs_actually_help: bool = Field(
        default=False,
        description="Docs actually help detection",
    )
    precision_gap: str | None = Field(
        default=None,
        description="Documentation precision gap",
    )


class MutationResult(VerificationResult):
    """Result of mutation detection evaluation.

    Attributes:
        team_a_evaluations: Evaluation of Team A assessments
        team_b_evaluations: Evaluation of Team B assessments
        detectable_mutations: Mutations detectable from both docs
        undetectable_mutations: Mutations neither could detect
        mutation_type_scores: Scores by mutation type
    """

    team_a_evaluations: list[MutationEvaluation] = Field(
        default_factory=list,
        description="Team A assessment evaluations",
    )
    team_b_evaluations: list[MutationEvaluation] = Field(
        default_factory=list,
        description="Team B assessment evaluations",
    )
    detectable_mutations: list[str] = Field(
        default_factory=list,
        description="Mutations detectable from both docs",
    )
    undetectable_mutations: list[str] = Field(
        default_factory=list,
        description="Mutations neither could detect",
    )
    mutation_type_scores: dict[str, float] = Field(
        default_factory=dict,
        description="Scores by mutation type",
    )

    def __init__(self, **data):
        if "strategy_type" not in data:
            data["strategy_type"] = StrategyType.MUTATION_DETECTION
        super().__init__(**data)


# =============================================================================
# Impact Analysis Models
# =============================================================================


class ImpactChallenge(VerificationChallenge):
    """Impact analysis challenge.

    Asks teams to predict what would break if a component changed.

    Attributes:
        change_type: Type of change proposed
        change_description: Description of the change
        actual_impacted: Components actually impacted (ground truth)
    """

    change_type: ChangeType = Field(
        ...,
        description="Type of proposed change",
    )
    change_description: str = Field(
        ...,
        description="Description of the change",
        examples=["Change calculate_total() signature: add required tax_rate param"],
    )
    actual_impacted: list[str] = Field(
        default_factory=list,
        description="Actually impacted components (ground truth)",
    )

    def __init__(self, **data):
        if "strategy_type" not in data:
            data["strategy_type"] = StrategyType.IMPACT_ANALYSIS
        super().__init__(**data)


class ImpactPrediction(BaseModel):
    """A team's prediction of impact.

    Attributes:
        predicted_impacted: Components predicted to need changes
        confidence: Team's confidence (0.0-1.0)
        reasoning: Explanation of prediction
    """

    predicted_impacted: list[str] = Field(
        default_factory=list,
        description="Predicted impacted components",
    )
    confidence: float = Field(
        default=0.5,
        ge=0.0,
        le=1.0,
        description="Prediction confidence",
    )
    reasoning: str = Field(
        default="",
        description="Explanation of prediction",
    )


class ImpactResult(VerificationResult):
    """Result of impact analysis evaluation.

    Attributes:
        team_a_prediction: Team A's prediction
        team_b_prediction: Team B's prediction
        team_a_true_positives: Correctly predicted by A
        team_a_false_positives: Overclaimed by A
        team_a_false_negatives: Missed by A (critical)
        team_b_true_positives: Correctly predicted by B
        team_b_false_positives: Overclaimed by B
        team_b_false_negatives: Missed by B (critical)
        team_a_precision: Precision of A's prediction
        team_a_recall: Recall of A's prediction
        team_b_precision: Precision of B's prediction
        team_b_recall: Recall of B's prediction
    """

    team_a_prediction: ImpactPrediction = Field(
        default_factory=ImpactPrediction,
        description="Team A's prediction",
    )
    team_b_prediction: ImpactPrediction = Field(
        default_factory=ImpactPrediction,
        description="Team B's prediction",
    )
    team_a_true_positives: list[str] = Field(
        default_factory=list,
        description="Correctly predicted by A",
    )
    team_a_false_positives: list[str] = Field(
        default_factory=list,
        description="Overclaimed by A",
    )
    team_a_false_negatives: list[str] = Field(
        default_factory=list,
        description="Missed by A",
    )
    team_b_true_positives: list[str] = Field(
        default_factory=list,
        description="Correctly predicted by B",
    )
    team_b_false_positives: list[str] = Field(
        default_factory=list,
        description="Overclaimed by B",
    )
    team_b_false_negatives: list[str] = Field(
        default_factory=list,
        description="Missed by B",
    )
    team_a_precision: float = Field(
        default=0.0,
        ge=0.0,
        le=1.0,
        description="A's prediction precision",
    )
    team_a_recall: float = Field(
        default=0.0,
        ge=0.0,
        le=1.0,
        description="A's prediction recall",
    )
    team_b_precision: float = Field(
        default=0.0,
        ge=0.0,
        le=1.0,
        description="B's prediction precision",
    )
    team_b_recall: float = Field(
        default=0.0,
        ge=0.0,
        le=1.0,
        description="B's prediction recall",
    )

    def __init__(self, **data):
        if "strategy_type" not in data:
            data["strategy_type"] = StrategyType.IMPACT_ANALYSIS
        super().__init__(**data)

    @computed_field
    @property
    def missed_by_both(self) -> list[str]:
        """Components missed by both teams - critical gaps."""
        return list(set(self.team_a_false_negatives) & set(self.team_b_false_negatives))


# =============================================================================
# Adversarial Review Models
# =============================================================================


class AdversarialFinding(BaseModel):
    """An issue found during adversarial review.

    When one team reviews the other's documentation against code.

    Attributes:
        finding_id: Unique identifier
        reviewed_team: Team whose docs were reviewed (A or B)
        issue_type: Type of issue found
        location: Where in documentation
        description: Description of the issue
        severity: Issue severity
        code_evidence: Evidence from code
    """

    finding_id: str = Field(
        ...,
        description="Unique identifier",
        examples=["finding_001"],
    )
    reviewed_team: str = Field(
        ...,
        description="Team reviewed (A or B)",
        pattern="^[AB]$",
    )
    issue_type: str = Field(
        ...,
        description="Type of issue",
        examples=["missing_param", "wrong_return_type", "incorrect_exception"],
    )
    location: str = Field(
        ...,
        description="Location in documentation",
    )
    description: str = Field(
        ...,
        description="Issue description",
    )
    severity: Severity = Field(
        default=Severity.MEDIUM,
        description="Issue severity",
    )
    code_evidence: str = Field(
        default="",
        description="Code evidence for the issue",
    )


class AdversarialChallenge(VerificationChallenge):
    """Adversarial review challenge.

    Teams review each other's documentation against source code.

    Attributes:
        team_a_documentation: Team A's documentation to review
        team_b_documentation: Team B's documentation to review
        source_code: Source code for verification
        max_findings: Maximum findings per review
    """

    team_a_documentation: str = Field(
        ...,
        description="Team A's documentation",
    )
    team_b_documentation: str = Field(
        ...,
        description="Team B's documentation",
    )
    source_code: str = Field(
        ...,
        description="Source code for verification",
    )
    max_findings: int = Field(
        default=10,
        ge=1,
        description="Max findings per review",
    )

    def __init__(self, **data):
        if "strategy_type" not in data:
            data["strategy_type"] = StrategyType.ADVERSARIAL_REVIEW
        super().__init__(**data)


class AdversarialResult(VerificationResult):
    """Result of adversarial review.

    Attributes:
        findings_by_a: Issues A found in B's docs
        findings_by_b: Issues B found in A's docs
        validated_findings: Findings confirmed against code
        false_findings: Findings that were incorrect
    """

    findings_by_a: list[AdversarialFinding] = Field(
        default_factory=list,
        description="Issues A found in B's docs",
    )
    findings_by_b: list[AdversarialFinding] = Field(
        default_factory=list,
        description="Issues B found in A's docs",
    )
    validated_findings: list[str] = Field(
        default_factory=list,
        description="Finding IDs confirmed by code",
    )
    false_findings: list[str] = Field(
        default_factory=list,
        description="Finding IDs that were incorrect",
    )

    def __init__(self, **data):
        if "strategy_type" not in data:
            data["strategy_type"] = StrategyType.ADVERSARIAL_REVIEW
        super().__init__(**data)

    @computed_field
    @property
    def total_valid_findings(self) -> int:
        """Total validated findings from both reviews."""
        return len(self.validated_findings)


# =============================================================================
# Test Generation Models
# =============================================================================


class GeneratedTest(BaseModel):
    """A test case generated from documentation.

    Attributes:
        test_id: Unique identifier
        test_name: Name of the test
        test_code: Test code (pytest format)
        tests_aspect: What aspect of behavior this tests
        from_team: Which team's docs generated this (A or B)
    """

    test_id: str = Field(
        ...,
        description="Unique identifier",
        examples=["test_001"],
    )
    test_name: str = Field(
        ...,
        description="Test name",
        examples=["test_process_data_empty_list"],
    )
    test_code: str = Field(
        ...,
        description="Test code",
    )
    tests_aspect: str = Field(
        ...,
        description="Aspect being tested",
        examples=["empty_input_handling", "exception_raised"],
    )
    from_team: str = Field(
        ...,
        description="Team whose docs generated this",
        pattern="^[AB]$",
    )


class TestGenerationChallenge(VerificationChallenge):
    """Test generation validation challenge.

    Generate tests from documentation and run against actual code.

    Attributes:
        team_a_documentation: Team A's documentation
        team_b_documentation: Team B's documentation
        source_code: Actual source code for test execution
        tests_per_team: Number of tests to generate per team
    """

    team_a_documentation: str = Field(
        ...,
        description="Team A's documentation",
    )
    team_b_documentation: str = Field(
        ...,
        description="Team B's documentation",
    )
    source_code: str = Field(
        ...,
        description="Source code for test execution",
    )
    tests_per_team: int = Field(
        default=10,
        ge=1,
        description="Tests to generate per team",
    )

    def __init__(self, **data):
        if "strategy_type" not in data:
            data["strategy_type"] = StrategyType.TEST_GENERATION
        super().__init__(**data)


class TestExecution(BaseModel):
    """Result of executing a generated test.

    Attributes:
        test_id: Test that was executed
        passed: Whether test passed
        error_message: Error if test failed
        failure_reason: Why documentation was wrong (if failed)
    """

    test_id: str = Field(
        ...,
        description="Test ID",
    )
    passed: bool = Field(
        default=False,
        description="Test passed",
    )
    error_message: str | None = Field(
        default=None,
        description="Error message if failed",
    )
    failure_reason: str | None = Field(
        default=None,
        description="Documentation error if test failed",
    )


class TestValidationResult(VerificationResult):
    """Result of test generation validation.

    Attributes:
        team_a_tests: Tests generated from A's docs
        team_b_tests: Tests generated from B's docs
        team_a_executions: Results of running A's tests
        team_b_executions: Results of running B's tests
        documentation_errors: Errors revealed by failed tests
    """

    team_a_tests: list[GeneratedTest] = Field(
        default_factory=list,
        description="Tests from A's docs",
    )
    team_b_tests: list[GeneratedTest] = Field(
        default_factory=list,
        description="Tests from B's docs",
    )
    team_a_executions: list[TestExecution] = Field(
        default_factory=list,
        description="A's test results",
    )
    team_b_executions: list[TestExecution] = Field(
        default_factory=list,
        description="B's test results",
    )
    documentation_errors: list[str] = Field(
        default_factory=list,
        description="Documentation errors found",
    )

    def __init__(self, **data):
        if "strategy_type" not in data:
            data["strategy_type"] = StrategyType.TEST_GENERATION
        super().__init__(**data)

    @computed_field
    @property
    def team_a_pass_rate(self) -> float:
        """Pass rate for Team A's generated tests."""
        if not self.team_a_executions:
            return 0.0
        passed = sum(1 for e in self.team_a_executions if e.passed)
        return passed / len(self.team_a_executions)

    @computed_field
    @property
    def team_b_pass_rate(self) -> float:
        """Pass rate for Team B's generated tests."""
        if not self.team_b_executions:
            return 0.0
        passed = sum(1 for e in self.team_b_executions if e.passed)
        return passed / len(self.team_b_executions)


# =============================================================================
# Code Reconstruction Models (Bonus Strategy)
# =============================================================================


class CodeReconstructionChallenge(VerificationChallenge):
    """Code reconstruction challenge.

    Given only documentation, reconstruct functionally equivalent code.

    Attributes:
        team_a_documentation: Team A's documentation
        team_b_documentation: Team B's documentation
        original_code: Original code (for evaluation only)
        function_signature: Signature to implement
    """

    team_a_documentation: str = Field(
        ...,
        description="Team A's documentation",
    )
    team_b_documentation: str = Field(
        ...,
        description="Team B's documentation",
    )
    original_code: str = Field(
        ...,
        description="Original code for evaluation",
    )
    function_signature: str = Field(
        ...,
        description="Function signature to implement",
    )

    def __init__(self, **data):
        if "strategy_type" not in data:
            data["strategy_type"] = StrategyType.CODE_RECONSTRUCTION
        super().__init__(**data)


class ReconstructedCode(BaseModel):
    """A team's reconstructed code.

    Attributes:
        from_team: Team that produced this (A or B)
        code: Reconstructed code
        unknown_areas: Areas where docs were insufficient
        assumptions_made: Assumptions made due to missing info
    """

    from_team: str = Field(
        ...,
        description="Team (A or B)",
        pattern="^[AB]$",
    )
    code: str = Field(
        ...,
        description="Reconstructed code",
    )
    unknown_areas: list[str] = Field(
        default_factory=list,
        description="Areas where docs were insufficient",
    )
    assumptions_made: list[str] = Field(
        default_factory=list,
        description="Assumptions made",
    )


class CodeReconstructionResult(VerificationResult):
    """Result of code reconstruction evaluation.

    Attributes:
        team_a_reconstruction: A's reconstructed code
        team_b_reconstruction: B's reconstructed code
        team_a_functional_equivalence: Whether A's code is equivalent
        team_b_functional_equivalence: Whether B's code is equivalent
        missing_from_both: Implementation details missing from both docs
    """

    team_a_reconstruction: ReconstructedCode = Field(
        ...,
        description="A's reconstruction",
    )
    team_b_reconstruction: ReconstructedCode = Field(
        ...,
        description="B's reconstruction",
    )
    team_a_functional_equivalence: float = Field(
        default=0.0,
        ge=0.0,
        le=1.0,
        description="A's functional equivalence",
    )
    team_b_functional_equivalence: float = Field(
        default=0.0,
        ge=0.0,
        le=1.0,
        description="B's functional equivalence",
    )
    missing_from_both: list[str] = Field(
        default_factory=list,
        description="Details missing from both docs",
    )

    def __init__(self, **data):
        if "strategy_type" not in data:
            data["strategy_type"] = StrategyType.CODE_RECONSTRUCTION
        super().__init__(**data)
